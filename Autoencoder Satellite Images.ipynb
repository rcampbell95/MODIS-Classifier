{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import gdal\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 9 --> 32), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(9, 32, 3, padding=1)  \n",
    "        # conv layer (depth from 32 --> 16), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(16, 32, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(32, 9, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        #x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = torch.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_image(image):\n",
    "    \"\"\"\n",
    "    Break a multispectral image into 9x4x4 tensors\n",
    "    image: Pytorch tensor with dimensions CxHxW where in this application\n",
    "        C is channels, H is height, and W is width\n",
    "    returns:\n",
    "        image: numpy array representing \n",
    "        ground_truth: \n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    height = image[0].shape[0]\n",
    "    width = image[0].shape[1]\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(0, height, 4):\n",
    "        band_chunks = []\n",
    "\n",
    "        if i + 3 > height - 1:\n",
    "            break\n",
    "\n",
    "        for j in range(0, width, 4):\n",
    "            rows = []\n",
    "            \n",
    "            if j + 4 > width:\n",
    "                break\n",
    "                \n",
    "            for band in image:\n",
    "                chunk = np.ndarray(shape=(0))\n",
    "                #try:\n",
    "                half1 = np.append([band[i + 0][j:j + 4]], [band[i + 1][j:j + 4]], axis=0)\n",
    "                half2 = np.append([band[i + 2][j:j + 4]], [band[i + 3][j:j + 4]], axis=0)\n",
    "                #except:\n",
    "                #print(height, width)\n",
    "                #print(i, j)\n",
    "                #break\n",
    "                chunk = np.append(half1, half2, axis=0)\n",
    "\n",
    "                rows.append(chunk)\n",
    "\n",
    "            band_chunks.append(rows)\n",
    "\n",
    "        data.append(band_chunks)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Satellite Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, filenames = load_data(10, \"./data/modis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [data[i][0] for i in range(len(data))]\n",
    "train_data = [data[i][1:] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "chunked_image = chunk_image(train_data[0])\n",
    "\n",
    "trainloader = DataLoader(chunked_image, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Instantiate Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:./mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.start_run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(ouput, expected):\n",
    "    loss_function = nn.MSELoss()\n",
    "    # Narrow fourth dimension, if uneven\n",
    "    #if output.size(3) != expected.size(2):\n",
    "    #    expected = torch.narrow(expected, 2, 1, output.size(3))\n",
    "    # Narrow third dimension, if uneven\n",
    "    #if output.size(2) != expected.size(1):\n",
    "    #    expected = torch.narrow(expected, 1, 1, output.size(2))\n",
    "    return loss_function(output, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Move Model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train the Autoenconder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "print_every = 16\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for index, batch in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Convert data to tensor and send tensor to GPU, if available\n",
    "        batch = batch.to(device)\n",
    "        batch = batch.view(-1, 9, 4, 4)\n",
    "        # torchvision.transforms.normalize(image, mean, std, inplace=False)\n",
    "        # Forward pass\n",
    "        output = model.forward(batch)\n",
    "        # Calculate loss \n",
    "        loss = criterion(output, batch)\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if (index + 1) % print_every == 0:\n",
    "    plt.imshow(output.squeeze().detach().numpy()[0])\n",
    "    plt.show()\n",
    "            #print(output.squeeze().detach().numpy().shape)\n",
    "        \n",
    "    avg_loss = train_loss/len(train_data)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        avg_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. References\n",
    "1. https://github.com/sjsu-earthquake-early-warning/deep-learning-v2-pytorch/blob/master/autoencoder/convolutional-autoencoder/Convolutional_Autoencoder_Solution.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
