{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, channel, in_len):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc1_size = channel * in_len ** 2\n",
    "        self.fc2_size = self.fc1_size * 3\n",
    "        self.fc3_size = self.fc2_size\n",
    "        self.fc4_size = self.fc1_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_size, self.fc2_size)\n",
    "        self.fc2 = nn.Linear(self.fc2_size, self.fc3_size)\n",
    "        self.fc3 = nn.Linear(self.fc3_size, self.fc4_size)\n",
    "        self.fc4 = nn.Linear(self.fc4_size, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, trainloader, val_loader, epochs=20):\n",
    "        pass\n",
    "                    \n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Start MlFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:.\\mlruns\")\n",
    "mlflow.start_run()\n",
    "\n",
    "params = {}\n",
    "artifacts = []\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, filenames = load_data(10, \"./data/modis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [data[i][0] for i in range(len(data))]\n",
    "train_data = [data[i][1:] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandsatDataLoader():\n",
    "    def __init__(self, data, ground_truth, batch_size, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __iter__(self):\n",
    "        self.len = 0\n",
    "        if self.shuffle:\n",
    "            import random\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        for i, image in enumerate(self.data):\n",
    "            chunked_data = chunk_image(merge_dims(image))\n",
    "            chunked_labels = chunk_image(self.labels[i], label=True)\n",
    "                        \n",
    "            dataset = list(zip(chunked_data, chunked_labels))\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "            self.len += len(dataloader)\n",
    "            \n",
    "            for batch, ground_truth in dataloader:\n",
    "                yield batch, ground_truth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Chunk Images\n",
    "Each image is broken up into bx9x3x3 tensors, where b is the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"{:30} shape: (batch, channel, height, width)\".format(\"filename\"))\n",
    "\n",
    "chunk_sum = 0\n",
    "\n",
    "for i, image in enumerate(train_data):\n",
    "    chunked_image = chunk_image(merge_dims(image))\n",
    "    \n",
    "    chunk_sum += chunked_image.shape[0]\n",
    "        \n",
    "    print(\"{:30} shape: {}\".format(filenames[i], chunked_image.shape))\n",
    "    \n",
    "print(\"\\nTotal {} x {} chunks: {}\".format(chunked_image.shape[-1], chunked_image.shape[-1], chunk_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "params[\"batch_size\"] = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = LandsatDataLoader(train_data[:12], labels[:12], batch_size=batch_size)\n",
    "val_loader = LandsatDataLoader(train_data[12:], labels[12:], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instantiate Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = parallelize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "params[\"epochs\"] = epochs\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "min_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, ground_truth in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        batch, ground_truth = batch.to(device), ground_truth.to(device)\n",
    "        output = model.forward(batch.float())\n",
    "        # Clear gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.squeeze(), ground_truth.long())\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            y_pred = np.array([])\n",
    "            y_true = np.array([])\n",
    "            \n",
    "            for batch, ground_truth in val_loader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                batch, ground_truth = batch.to(device), ground_truth.to(device)\n",
    "                # forward pass\n",
    "                log_probs = model.forward(batch.float())\n",
    "                probs = torch.exp(log_probs)\n",
    "                \n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                y_pred = np.append(y_pred, cuda_to_numpy(top_class))\n",
    "                y_true = np.append(y_true, cuda_to_numpy(ground_truth))\n",
    "                \n",
    "                # calculate loss\n",
    "                loss = criterion(log_probs.squeeze(), ground_truth.long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = val_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if v_loss_avg < min_val_loss:\n",
    "        torch.save(model.state_dict(), \"./artifacts/model.pth\")\n",
    "        artifacts.append(\"model.pth\")\n",
    "        \n",
    "    mlflow.log_metric(\"train_loss\", t_loss_avg)\n",
    "    mlflow.log_metric(\"val_loss\", v_loss_avg)\n",
    "    mlflow.log_metric(\"validation_accuracy\", accuracy)\n",
    "    \n",
    "    train_losses.append(t_loss_avg)\n",
    "    val_losses.append(v_loss_avg)\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:8.6f} | validation loss: {:8.6f} | validation accuracy: {:6.4f}%'.format(\n",
    "                epoch+1, epochs, t_loss_avg, v_loss_avg, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve for MODIS Image Classifier\")\n",
    "plt.legend()\n",
    "\n",
    "figure_name = \"train_loss.png\"\n",
    "plt.savefig(\"./artifacts/\" + figure_name)\n",
    "artifacts.append(figure_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy(%)\")\n",
    "plt.title(\"Validation Accuracy for MODIS Image Classifier\")\n",
    "\n",
    "figure_name = \"val_accuracy.png\"\n",
    "plt.savefig(\"./artifacts/\" + figure_name)\n",
    "artifacts.append(figure_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Wrap up MlFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in params.items():\n",
    "    mlflow.log_param(name, val)\n",
    "\n",
    "for name, val in metrics.items():\n",
    "    mlflow.log_metric(name, val)\n",
    "    \n",
    "artifact_path = \"./artifacts/\"\n",
    "for name in artifacts:\n",
    "    mlflow.log_artifact(artifact_path + name)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
