{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, channel, in_len):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc1_size = channel * in_len ** 2\n",
    "        self.fc2_size = self.fc1_size * 2\n",
    "        self.fc3_size = self.fc2_size\n",
    "        self.fc4_size = self.fc1_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_size, self.fc2_size)\n",
    "        self.fc2 = nn.Linear(self.fc2_size, self.fc3_size)\n",
    "        self.fc3 = nn.Linear(self.fc3_size, self.fc4_size)\n",
    "        self.fc4 = nn.Linear(self.fc4_size, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, trainloader, val_loader, epochs=20):\n",
    "        pass\n",
    "                    \n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Start MlFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:.\\mlruns\")\n",
    "mlflow.start_run()\n",
    "\n",
    "params = {}\n",
    "artifacts = []\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, filenames = load_data(10, \"./data/modis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [data[i][0] for i in range(len(data))]\n",
    "train_data = [data[i][1:] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Chunk Images\n",
    "Each image is broken up into bx9x3x3 tensors, where b is the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename                       shape: (batch, channel, height, width)\n",
      "arkansas_city.tif              shape: (556850, 9, 3, 3)\n",
      "assiniboine.tif                shape: (2182830, 9, 3, 3)\n",
      "bay_area.tif                   shape: (3165612, 9, 3, 3)\n",
      "berkeley.tif                   shape: (431616, 9, 3, 3)\n",
      "kashmore.tif                   shape: (3304125, 9, 3, 3)\n",
      "kashmore_north.tif             shape: (328040, 9, 3, 3)\n",
      "katrina.tif                    shape: (1263893, 9, 3, 3)\n",
      "katrina_slidell.tif            shape: (249676, 9, 3, 3)\n",
      "malawi.tif                     shape: (423504, 9, 3, 3)\n",
      "mississippi_june.tif           shape: (824680, 9, 3, 3)\n",
      "mississippi_may.tif            shape: (824680, 9, 3, 3)\n",
      "parana.tif                     shape: (686907, 9, 3, 3)\n",
      "sava.tif                       shape: (753087, 9, 3, 3)\n",
      "sava_west.tif                  shape: (410116, 9, 3, 3)\n",
      "unflooded_mississippi.tif      shape: (1662630, 9, 3, 3)\n",
      "unflooded_new_orleans.tif      shape: (1403010, 9, 3, 3)\n",
      "\n",
      "Total 3 x 3 chunks: 18471256\n",
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"{:30} shape: (batch, channel, height, width)\".format(\"filename\"))\n",
    "\n",
    "chunk_sum = 0\n",
    "\n",
    "#datasets = []\n",
    "\n",
    "for index, image in enumerate(train_data):\n",
    "    chunked_image = chunk_image(merge_dims(image))\n",
    "    #chunked_label = chunk_image(labels[index], label=True)\n",
    "    \n",
    "    chunk_sum += chunked_image.shape[0]\n",
    "    \n",
    "    #datasets.append(torch.utils.data.TensorDataset(torch.from_numpy(chunked_image), torch.from_numpy(chunked_label)))\n",
    "    \n",
    "    print(\"{:30} shape: {}\".format(filenames[index], chunked_image.shape))\n",
    "    \n",
    "print(\"\\nTotal {} x {} chunks: {}\".format(chunked_image.shape[-1], chunked_image.shape[-1], chunk_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556850,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "params[\"train_image\"] = filenames[i]\n",
    "\n",
    "chunked_data = chunk_image(merge_dims(train_data[i]))\n",
    "chunked_labels = chunk_image(labels[i], label=True)\n",
    "chunked_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "params[\"batch_size\"] = batch_size\n",
    "\n",
    "trainloader, val_loader = split_trainset(chunked_data, chunked_labels, ratio=0.7, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instantiate Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = parallelize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=81, out_features=162, bias=True)\n",
       "  (fc2): Linear(in_features=162, out_features=162, bias=True)\n",
       "  (fc3): Linear(in_features=162, out_features=81, bias=True)\n",
       "  (fc4): Linear(in_features=81, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/   70] | train loss: 0.2563 | validation loss: 0.1772 | validation accuracy: 92.6946%\n",
      "Epoch [    2/   70] | train loss: 0.1824 | validation loss: 0.1730 | validation accuracy: 92.7641%\n",
      "Epoch [    3/   70] | train loss: 0.1782 | validation loss: 0.1698 | validation accuracy: 92.7778%\n",
      "Epoch [    4/   70] | train loss: 0.1752 | validation loss: 0.1679 | validation accuracy: 92.8503%\n",
      "Epoch [    5/   70] | train loss: 0.1719 | validation loss: 0.1657 | validation accuracy: 92.9311%\n",
      "Epoch [    6/   70] | train loss: 0.1699 | validation loss: 0.1641 | validation accuracy: 92.8880%\n",
      "Epoch [    7/   70] | train loss: 0.1687 | validation loss: 0.1647 | validation accuracy: 92.8640%\n",
      "Epoch [    8/   70] | train loss: 0.1680 | validation loss: 0.1607 | validation accuracy: 93.0257%\n",
      "Epoch [    9/   70] | train loss: 0.1663 | validation loss: 0.1621 | validation accuracy: 93.0484%\n",
      "Epoch [   10/   70] | train loss: 0.1649 | validation loss: 0.1585 | validation accuracy: 93.2920%\n",
      "Epoch [   11/   70] | train loss: 0.1638 | validation loss: 0.1568 | validation accuracy: 93.2950%\n",
      "Epoch [   12/   70] | train loss: 0.1621 | validation loss: 0.1558 | validation accuracy: 93.3639%\n",
      "Epoch [   13/   70] | train loss: 0.1612 | validation loss: 0.1549 | validation accuracy: 93.3040%\n",
      "Epoch [   14/   70] | train loss: 0.1607 | validation loss: 0.1532 | validation accuracy: 93.4572%\n",
      "Epoch [   15/   70] | train loss: 0.1600 | validation loss: 0.1554 | validation accuracy: 93.2304%\n",
      "Epoch [   16/   70] | train loss: 0.1590 | validation loss: 0.1552 | validation accuracy: 93.2908%\n",
      "Epoch [   17/   70] | train loss: 0.1583 | validation loss: 0.1515 | validation accuracy: 93.4985%\n",
      "Epoch [   18/   70] | train loss: 0.1577 | validation loss: 0.1511 | validation accuracy: 93.4279%\n",
      "Epoch [   19/   70] | train loss: 0.1564 | validation loss: 0.1505 | validation accuracy: 93.5213%\n",
      "Epoch [   20/   70] | train loss: 0.1558 | validation loss: 0.1496 | validation accuracy: 93.3956%\n",
      "Epoch [   21/   70] | train loss: 0.1555 | validation loss: 0.1529 | validation accuracy: 93.3315%\n",
      "Epoch [   22/   70] | train loss: 0.1551 | validation loss: 0.1478 | validation accuracy: 93.5129%\n",
      "Epoch [   23/   70] | train loss: 0.1545 | validation loss: 0.1473 | validation accuracy: 93.5602%\n",
      "Epoch [   24/   70] | train loss: 0.1537 | validation loss: 0.1471 | validation accuracy: 93.5752%\n",
      "Epoch [   25/   70] | train loss: 0.1534 | validation loss: 0.1467 | validation accuracy: 93.5327%\n",
      "Epoch [   26/   70] | train loss: 0.1527 | validation loss: 0.1461 | validation accuracy: 93.4746%\n",
      "Epoch [   27/   70] | train loss: 0.1530 | validation loss: 0.1461 | validation accuracy: 93.5889%\n",
      "Epoch [   28/   70] | train loss: 0.1528 | validation loss: 0.1468 | validation accuracy: 93.5087%\n",
      "Epoch [   29/   70] | train loss: 0.1520 | validation loss: 0.1476 | validation accuracy: 93.6177%\n",
      "Epoch [   30/   70] | train loss: 0.1519 | validation loss: 0.1455 | validation accuracy: 93.5351%\n",
      "Epoch [   31/   70] | train loss: 0.1515 | validation loss: 0.1451 | validation accuracy: 93.5853%\n",
      "Epoch [   32/   70] | train loss: 0.1518 | validation loss: 0.1469 | validation accuracy: 93.5363%\n",
      "Epoch [   33/   70] | train loss: 0.1515 | validation loss: 0.1465 | validation accuracy: 93.5141%\n",
      "Epoch [   34/   70] | train loss: 0.1517 | validation loss: 0.1458 | validation accuracy: 93.5722%\n",
      "Epoch [   35/   70] | train loss: 0.1505 | validation loss: 0.1440 | validation accuracy: 93.6404%\n",
      "Epoch [   36/   70] | train loss: 0.1507 | validation loss: 0.1441 | validation accuracy: 93.6003%\n",
      "Epoch [   37/   70] | train loss: 0.1508 | validation loss: 0.1436 | validation accuracy: 93.5291%\n",
      "Epoch [   38/   70] | train loss: 0.1501 | validation loss: 0.1449 | validation accuracy: 93.5716%\n",
      "Epoch [   39/   70] | train loss: 0.1502 | validation loss: 0.1445 | validation accuracy: 93.6440%\n",
      "Epoch [   40/   70] | train loss: 0.1504 | validation loss: 0.1456 | validation accuracy: 93.6039%\n",
      "Epoch [   41/   70] | train loss: 0.1508 | validation loss: 0.1438 | validation accuracy: 93.6476%\n",
      "Epoch [   42/   70] | train loss: 0.1498 | validation loss: 0.1446 | validation accuracy: 93.5237%\n",
      "Epoch [   43/   70] | train loss: 0.1498 | validation loss: 0.1437 | validation accuracy: 93.6656%\n",
      "Epoch [   44/   70] | train loss: 0.1497 | validation loss: 0.1442 | validation accuracy: 93.5967%\n",
      "Epoch [   45/   70] | train loss: 0.1500 | validation loss: 0.1425 | validation accuracy: 93.6638%\n",
      "Epoch [   46/   70] | train loss: 0.1493 | validation loss: 0.1436 | validation accuracy: 93.6494%\n",
      "Epoch [   47/   70] | train loss: 0.1490 | validation loss: 0.1435 | validation accuracy: 93.6979%\n",
      "Epoch [   48/   70] | train loss: 0.1493 | validation loss: 0.1433 | validation accuracy: 93.6536%\n",
      "Epoch [   49/   70] | train loss: 0.1488 | validation loss: 0.1431 | validation accuracy: 93.4908%\n",
      "Epoch [   50/   70] | train loss: 0.1485 | validation loss: 0.1413 | validation accuracy: 93.7206%\n",
      "Epoch [   51/   70] | train loss: 0.1485 | validation loss: 0.1427 | validation accuracy: 93.6482%\n",
      "Epoch [   52/   70] | train loss: 0.1487 | validation loss: 0.1422 | validation accuracy: 93.7290%\n",
      "Epoch [   53/   70] | train loss: 0.1477 | validation loss: 0.1412 | validation accuracy: 93.7069%\n",
      "Epoch [   54/   70] | train loss: 0.1479 | validation loss: 0.1423 | validation accuracy: 93.6302%\n",
      "Epoch [   55/   70] | train loss: 0.1478 | validation loss: 0.1421 | validation accuracy: 93.6237%\n",
      "Epoch [   56/   70] | train loss: 0.1479 | validation loss: 0.1413 | validation accuracy: 93.6805%\n",
      "Epoch [   57/   70] | train loss: 0.1474 | validation loss: 0.1416 | validation accuracy: 93.6368%\n",
      "Epoch [   58/   70] | train loss: 0.1478 | validation loss: 0.1418 | validation accuracy: 93.6751%\n",
      "Epoch [   59/   70] | train loss: 0.1480 | validation loss: 0.1422 | validation accuracy: 93.7063%\n",
      "Epoch [   60/   70] | train loss: 0.1473 | validation loss: 0.1420 | validation accuracy: 93.7051%\n",
      "Epoch [   61/   70] | train loss: 0.1477 | validation loss: 0.1431 | validation accuracy: 93.6805%\n",
      "Epoch [   62/   70] | train loss: 0.1475 | validation loss: 0.1407 | validation accuracy: 93.7194%\n",
      "Epoch [   63/   70] | train loss: 0.1474 | validation loss: 0.1405 | validation accuracy: 93.7673%\n",
      "Epoch [   64/   70] | train loss: 0.1473 | validation loss: 0.1405 | validation accuracy: 93.7589%\n",
      "Epoch [   65/   70] | train loss: 0.1467 | validation loss: 0.1405 | validation accuracy: 93.7631%\n",
      "Epoch [   66/   70] | train loss: 0.1473 | validation loss: 0.1430 | validation accuracy: 93.7589%\n",
      "Epoch [   67/   70] | train loss: 0.1465 | validation loss: 0.1405 | validation accuracy: 93.7625%\n",
      "Epoch [   68/   70] | train loss: 0.1466 | validation loss: 0.1409 | validation accuracy: 93.7152%\n"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "params[\"epochs\"] = epochs\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "min_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, ground_truth in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        batch, ground_truth = batch.to(device), ground_truth.to(device)\n",
    "        output = model.forward(batch.float())\n",
    "        # Clear gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.squeeze(), ground_truth.long())\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            y_pred = np.array([])\n",
    "            y_true = np.array([])\n",
    "            \n",
    "            for batch, ground_truth in val_loader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                batch, ground_truth = batch.to(device), ground_truth.to(device)\n",
    "                # forward pass\n",
    "                log_probs = model.forward(batch.float())\n",
    "                probs = torch.exp(log_probs)\n",
    "                \n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                y_pred = np.append(y_pred, cuda_to_numpy(top_class))\n",
    "                y_true = np.append(y_true, cuda_to_numpy(ground_truth))\n",
    "                \n",
    "                # calculate loss\n",
    "                loss = criterion(log_probs.squeeze(), ground_truth.long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = val_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if v_loss_avg < min_val_loss:\n",
    "        torch.save(model.state_dict(), \"./artifacts/model.pth\")\n",
    "        artifacts.append(\"model.pth\")\n",
    "        \n",
    "    mlflow.log_metric(\"train_loss\", t_loss_avg)\n",
    "    mlflow.log_metric(\"val_loss\", v_loss_avg)\n",
    "    mlflow.log_metric(\"validation_accuracy\", accuracy)\n",
    "    \n",
    "    train_losses.append(t_loss_avg)\n",
    "    val_losses.append(v_loss_avg)\n",
    "    val_accuracies.append(accuracy)\n",
    "       \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:6.4f} | validation loss: {:6.4f} | validation accuracy: {:6.4f}%'.format(\n",
    "                epoch+1, epochs, t_loss_avg, v_loss_avg, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve for MODIS Image Classifier\")\n",
    "plt.legend()\n",
    "\n",
    "figure_name = \"train_loss.png\"\n",
    "plt.savefig(\"./artifacts/\" + figure_name)\n",
    "artifacts.append(figure_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy(%)\")\n",
    "plt.title(\"Validation Accuracy for MODIS Image Classifier\")\n",
    "\n",
    "figure_name = \"val_accuracy.png\"\n",
    "plt.savefig(\"./artifacts/\" + figure_name)\n",
    "artifacts.append(figure_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Wrap up MlFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in params.items():\n",
    "    mlflow.log_param(name, val)\n",
    "\n",
    "for name, val in metrics.items():\n",
    "    mlflow.log_metric(name, val)\n",
    "    \n",
    "artifact_path = \"./artifacts/\"\n",
    "for name in artifacts:\n",
    "    mlflow.log_artifact(artifact_path + name)\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
